{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "017bc9d0-c55a-4f54-bf6a-478a50abb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, SpatialDropout2D, Activation, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "import keras\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9ac48-d408-4ffb-ad32-f9bf9255f7db",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fcc041f-12ff-4324-b740-4db71cab0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_segmentation_mask(segmentation_mask):\n",
    "    \"\"\"preprocess the semgentation mask\n",
    "\n",
    "    The original segmentation mask has three categories.\n",
    "      foreground, background and outline\n",
    "    This function will just convert it to foreground and background \n",
    "\n",
    "    The original segmentation mask is also 1-index, so will convert it\n",
    "    to 0-index.\n",
    "\n",
    "    the original mask is represented as:\n",
    "    1 - edge of dog/cat and things like leashes etc.\n",
    "    2 - background\n",
    "    3 - foreground\n",
    "\n",
    "    we want to just keep the merge the edges and foreground of the doggo/catto, and\n",
    "    then treat it as a binary semantic segmentation task.\n",
    "    To achieve this, we will just subtract two, converting to values of [-1, 0, 1],\n",
    "    and then apply the abs function to convert the -1 values (edges) to the foreground.\n",
    "\n",
    "    Will also convert it to 32 bit float which will be needed for working with tf.\n",
    "    \n",
    "    Why am I doing it this way?\n",
    "     A reasonable question. Initially I tried to do it with just normal array indexing,\n",
    "     but this is a bit more work since the mask is a tensorflow tensor and not a np array.\n",
    "     We could alternatively convert it to an array, perform indexing and then map it back,\n",
    "     but this would have a performance overhead, which wouldn't be a big deal, but still.\n",
    "     With all that being said, I am doing it for you, so you don't have to.\n",
    "\n",
    "    Args:\n",
    "      segmentation_mask (array):\n",
    "        original segmentation mask\n",
    "\n",
    "    Returns:\n",
    "      preprocessed segmentation_mask\n",
    "    \"\"\"\n",
    "    return tf.abs(tf.cast(segmentation_mask, tf.float32) - 2)\n",
    "\n",
    "def return_image_label_mask(ds_out):\n",
    "    \"\"\" function to return image, class label and segmentation mask\n",
    "\n",
    "    The original dataset contains additional information, such as the filename and\n",
    "    the species. We don't care about any of that for this work, so will\n",
    "    discard them and just keep the original image as our input, and then\n",
    "    a tuple of our outputs that will be the class label and the semantic\n",
    "    segmentation mask.\n",
    "\n",
    "    Whilst we are here, we will also preprocess the segmentation mask.\n",
    "\n",
    "    Args:\n",
    "      ds_out: dict\n",
    "        original dataset output\n",
    "\n",
    "    Returns:\n",
    "       RGB image\n",
    "       tuple of class label and preprocessed segmentation mask\n",
    "    \"\"\"\n",
    "    # preprocess the segmentation mask\n",
    "    seg_mask =  preprocess_segmentation_mask(ds_out['segmentation_mask'])\n",
    "    image = tf.cast(ds_out['image'], tf.float32)\n",
    "    # image = standardise_image(image)\n",
    "    return image, (ds_out['label'], seg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46efc3cc-d7f9-4cee-b541-8640cc458969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def random_rotations(x, y, max_rotation=0.785398):\n",
    "    \"\"\"apply random rotations to predictor variable and my autoencoder target\"\"\"\n",
    "    rot_val = tf.random.uniform((), minval=-max_rotation, maxval=max_rotation)\n",
    "    image = tfa.image.rotate(x, rot_val, fill_mode='nearest')\n",
    "    mask = tfa.image.rotate(y[1], rot_val, fill_mode='nearest')\n",
    "    return image, (y[0], mask)\n",
    "\n",
    "def load_oxford_pets(split,\n",
    "                     batch_size=233,\n",
    "                     shuffle=True,\n",
    "                     image_size=300):\n",
    "    \"\"\"Load Oxford pets dataset for Assignment 1B\n",
    "\n",
    "    Function handles loading of data for 1b, included processing of images and\n",
    "    semantic segmentation masks. This function will\n",
    "    organise the tensorflow dataset to return an output that is a tuple, where\n",
    "    the tuple will be (classification_labels, segmentation_masks).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : string\n",
    "        either train or test string\n",
    "    batch_size : int\n",
    "        size of batches to use\n",
    "    shuffle : bool\n",
    "        whether to shuffle the dataset (WILL ONLY APPLY TO TRAIN)\n",
    "    image_size : int\n",
    "        new image size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "       tf.Dataset containing the Oxford pets dataset\n",
    "    \"\"\"\n",
    "    # lets do some error checking first\n",
    "    if (split != 'train') and (split != 'test'):\n",
    "        raise ValueError('Arg for split must be either \\'train\\' or \\'test\\'')\n",
    "    if (split == 'test') and shuffle:\n",
    "        print(\"WARNING: shuffle is set to true, but have specified split to be \\'test\\'\")\n",
    "        print('The shuffle argument will be ignored')\n",
    "        shuffle = False\n",
    "   \n",
    "    # now start loading the dataset\n",
    "    ds = tfds.load('oxford_iiit_pet',\n",
    "                   split=split,\n",
    "                   with_info=False)\n",
    "    # remove unnecessary dataset info\n",
    "    ds = ds.map(return_image_label_mask)\n",
    "\n",
    "    # Apply random rotations\n",
    "    ds = ds.map(random_rotations)\n",
    "    \n",
    "    # Final processing of the data \n",
    "    # here we will resize the data, and add the preprocessing that is needed \n",
    "    # and compatable with the mobilenet models.\n",
    "    ds = ds.map(lambda inp, out: preprocess_and_resize(inp, out, image_size))\n",
    "\n",
    "    if split == 'train' and shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fe1bbb1-7f7b-4dc3-81b8-6c309458b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_resize(image, output, image_size):\n",
    "    \"\"\"apply preprocessing steps above to images and resize images and maps\n",
    "    \n",
    "    Each image in the dataset is of a different size. The resizing will make sure\n",
    "    each image is the same size.\n",
    "    \"\"\"\n",
    "    # resize the image and the semantic segmentation mask\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    image = mobilenet_preprocess_image(image)\n",
    "    mask = tf.image.resize(output[1], [image_size, image_size])\n",
    "    return image, (output[0], mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1c935d6-363d-4184-b3bc-08d7361e0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_preprocess_image(image):\n",
    "    \"\"\"Apply preprocessing that is suitable for MobileNetV3.\n",
    "    \n",
    "    Simply scales to ranges [-1, 1]\n",
    "    \n",
    "    \n",
    "    you should use this preprocessing for both your model and the mobilenet model\n",
    "    \"\"\"\n",
    "    image = (image - 127.5) / 255.0\n",
    "    return image\n",
    "    \n",
    "    \n",
    "def unprocess_image(image):\n",
    "    \"\"\" undo preprocessing above so can plot images\"\"\"\n",
    "    image = image * 255.0 + 127.5\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a02a4a3-e1e3-45fb-af9e-f9e7a97573e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it and plotting some images.\n",
    "# NOTE: the image size I am setting here is all but definitely too large. You will need\n",
    "# to chage this yourself to something that is suitable given your constraints\n",
    "# NOTE: The batch size is also too large. I am doing this on purpose to force you to \n",
    "# pick a suitable batch size yourself\n",
    "image_size = 300\n",
    "batch_size = 273\n",
    "train_class_seg = load_oxford_pets('train', shuffle=True, batch_size=batch_size, image_size=image_size)\n",
    "test_class_seg = load_oxford_pets('test', shuffle=False, batch_size=batch_size, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff06acfa-2476-4042-869c-59c1bb1f4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_and_mask(image, outputs):\n",
    "    resized_image = tf.image.resize(image, [128, 128])\n",
    "    resized_mask = tf.image.resize(outputs[1], [128, 128])\n",
    "    return resized_image, (outputs[0], resized_mask)\n",
    "\n",
    "train_class_seg = train_dataset.map(resize_image_and_mask)\n",
    "val_class_seg = val_dataset.map(resize_image_and_mask)\n",
    "test_class_seg = test_class_seg.map(resize_image_and_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf973fba-12e7-4afb-a1df-b08bdd39ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_class_label(image, outputs):\n",
    "    return image, outputs[1]  # Return only the image and mask\n",
    "\n",
    "train_class_seg = train_class_seg.map(exclude_class_label)\n",
    "val_class_seg = val_class_seg.map(exclude_class_label)\n",
    "test_class_seg = test_class_seg.map(exclude_class_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154866b-25fa-4f0d-9b03-b9ea800d5c76",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8a50510-0117-467a-a7f6-24d6deb07344",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9de14a7f-ab9a-4ed9-ab8a-87900d6453ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, callbacks\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pre-trained MobileNetV3\n",
    "mobile_base = keras.applications.MobileNetV3Small(input_shape=(image_size, image_size, 3),\n",
    "                                                  include_top=False,\n",
    "                                                  include_preprocessing=False)\n",
    "\n",
    "# Set up the base network / encoder to get embedding\n",
    "inputs = layers.Input((128, 128, 3))\n",
    "embedding = mobile_base(inputs)\n",
    "\n",
    "# Classification output\n",
    "flattened = layers.GlobalAveragePooling2D(name='gap')(embedding)\n",
    "classification = layers.Dense(37, activation='softmax', name='classification')(flattened)\n",
    "\n",
    "# Decoder\n",
    "x = layers.UpSampling2D((3, 3))(embedding)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "x = layers.UpSampling2D((3, 3))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "x = layers.UpSampling2D((3, 3))(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.SpatialDropout2D(0.2)(x)\n",
    "\n",
    "# Output for binary semantic segmentation - one channel\n",
    "x = layers.Conv2D(1, (1, 1), activation=None, padding='same')(x)\n",
    "segmentation = layers.Resizing(128, 128, name='segmentation')(x)\n",
    "\n",
    "# Define model\n",
    "model5 = models.Model(inputs=inputs, outputs=[classification, segmentation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f212ce8-f27d-487c-80c1-ae0ef75ee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class CustomSaveCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filepath, validation_data, epoch_offset=0):\n",
    "        super(CustomSaveCallback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.validation_data = validation_data\n",
    "        self.epoch_offset = epoch_offset\n",
    "        \n",
    "        # Check if file already exists\n",
    "        file_exists = os.path.exists(f\"{self.filepath}/accuracies.csv\")\n",
    "        \n",
    "        # Create or append to the CSV file\n",
    "        file_mode = \"a\" if file_exists else \"w\"\n",
    "        with open(f\"{self.filepath}/accuracies.csv\", file_mode, newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # If it's a new file, write the header\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"Epoch\", \"Segmentation Accuracy\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Adjust epoch with the offset\n",
    "        adjusted_epoch = epoch + self.epoch_offset\n",
    "    \n",
    "        # Save the model\n",
    "        self.model.save(f\"{self.filepath}/model_at_epoch_{adjusted_epoch + 1}.h5\")\n",
    "        \n",
    "        # Predict the segmentation masks using the model\n",
    "        _, seg_preds = self.model.predict(self.validation_data)\n",
    "        \n",
    "        # Extract true masks from the validation data\n",
    "        true_masks = np.concatenate([y[1].numpy() for x, y in self.validation_data], axis=0)\n",
    "    \n",
    "        seg_preds_flat = (seg_preds.reshape(-1) > 0.5).astype(int)\n",
    "        true_masks_flat = (true_masks.reshape(-1) > 0.5).astype(int)\n",
    "    \n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(true_masks_flat, seg_preds_flat)\n",
    "    \n",
    "        # Retrieve the segmentation accuracy from logs\n",
    "        seg_accuracy = logs['segmentation_accuracy']\n",
    "    \n",
    "        # Check if the combined CSV already exists\n",
    "        file_exists = os.path.exists(f\"{self.filepath}/segmentation_metrics_epoch_{adjusted_epoch + 1}.csv\")\n",
    "    \n",
    "        # Write to the combined CSV\n",
    "        file_mode = \"a\" if file_exists else \"w\"\n",
    "        with open(f\"{self.filepath}/segmentation_metrics_epoch_{adjusted_epoch + 1}.csv\", file_mode, newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # If it's a new file, write the header\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"Epoch\", \"Segmentation Accuracy\", \"True Positives\", \"False Positives\", \"False Negatives\", \"True Negatives\"])\n",
    "            # Assuming the confusion matrix structure: [[TP, FP], [FN, TN]]\n",
    "            writer.writerow([adjusted_epoch + 1, seg_accuracy, cm[0,0], cm[0,1], cm[1,0], cm[1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "875151e4-eb3b-4699-905b-6f3c50805de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "image_size =  128\n",
    "train_class_seg = load_oxford_pets('train', shuffle=True, batch_size=273, image_size=image_size)\n",
    "test_class_seg = load_oxford_pets('test', shuffle=False, batch_size=batch_size, image_size=image_size)\n",
    "\n",
    "train_ratio = 0.8  # 80% for training, 20% for validation\n",
    "train_size = int(train_ratio * len(train_class_seg))\n",
    "val_size = len(train_class_seg) - train_size\n",
    "\n",
    "print(train_size)\n",
    "print(val_size)\n",
    "\n",
    "train_dataset = train_class_seg.take(train_size)\n",
    "val_dataset = train_class_seg.skip(train_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd3d7deb-2b8a-45d9-be03-3c23ee50429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 4.2947 - classification_loss: 3.7167 - segmentation_loss: 0.5780 - classification_accuracy: 0.0696 - segmentation_accuracy: 0.7215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 4.25500, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 13s 886ms/step\n",
      "11/11 [==============================] - 58s 5s/step - loss: 4.2947 - classification_loss: 3.7167 - segmentation_loss: 0.5780 - classification_accuracy: 0.0696 - segmentation_accuracy: 0.7215 - val_loss: 4.2550 - val_classification_loss: 3.3610 - val_segmentation_loss: 0.8940 - val_classification_accuracy: 0.1093 - val_segmentation_accuracy: 0.7165\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 3.4910 - classification_loss: 3.0708 - segmentation_loss: 0.4202 - classification_accuracy: 0.1832 - segmentation_accuracy: 0.8142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 4.25500 to 3.52035, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 867ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 3.4910 - classification_loss: 3.0708 - segmentation_loss: 0.4202 - classification_accuracy: 0.1832 - segmentation_accuracy: 0.8142 - val_loss: 3.5204 - val_classification_loss: 2.7570 - val_segmentation_loss: 0.7634 - val_classification_accuracy: 0.2703 - val_segmentation_accuracy: 0.8008\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.9713 - classification_loss: 2.5961 - segmentation_loss: 0.3752 - classification_accuracy: 0.3107 - segmentation_accuracy: 0.8371"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 3.52035 to 2.93417, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 857ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 2.9713 - classification_loss: 2.5961 - segmentation_loss: 0.3752 - classification_accuracy: 0.3107 - segmentation_accuracy: 0.8371 - val_loss: 2.9342 - val_classification_loss: 2.3793 - val_segmentation_loss: 0.5549 - val_classification_accuracy: 0.3560 - val_segmentation_accuracy: 0.8400\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.5985 - classification_loss: 2.2429 - segmentation_loss: 0.3556 - classification_accuracy: 0.4109 - segmentation_accuracy: 0.8443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 2.93417 to 2.54093, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 836ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 2.5985 - classification_loss: 2.2429 - segmentation_loss: 0.3556 - classification_accuracy: 0.4109 - segmentation_accuracy: 0.8443 - val_loss: 2.5409 - val_classification_loss: 2.0929 - val_segmentation_loss: 0.4480 - val_classification_accuracy: 0.4727 - val_segmentation_accuracy: 0.8472\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.3212 - classification_loss: 1.9754 - segmentation_loss: 0.3458 - classification_accuracy: 0.4772 - segmentation_accuracy: 0.8472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 2.54093 to 2.18265, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 13s 878ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 2.3212 - classification_loss: 1.9754 - segmentation_loss: 0.3458 - classification_accuracy: 0.4772 - segmentation_accuracy: 0.8472 - val_loss: 2.1826 - val_classification_loss: 1.7883 - val_segmentation_loss: 0.3943 - val_classification_accuracy: 0.5524 - val_segmentation_accuracy: 0.8587\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.1351 - classification_loss: 1.8019 - segmentation_loss: 0.3333 - classification_accuracy: 0.5178 - segmentation_accuracy: 0.8509"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 2.18265 to 2.05142, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 13s 872ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 2.1351 - classification_loss: 1.8019 - segmentation_loss: 0.3333 - classification_accuracy: 0.5178 - segmentation_accuracy: 0.8509 - val_loss: 2.0514 - val_classification_loss: 1.7191 - val_segmentation_loss: 0.3323 - val_classification_accuracy: 0.5539 - val_segmentation_accuracy: 0.8618\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.9926 - classification_loss: 1.6692 - segmentation_loss: 0.3234 - classification_accuracy: 0.5468 - segmentation_accuracy: 0.8542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 2.05142 to 1.87886, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 836ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.9926 - classification_loss: 1.6692 - segmentation_loss: 0.3234 - classification_accuracy: 0.5468 - segmentation_accuracy: 0.8542 - val_loss: 1.8789 - val_classification_loss: 1.5379 - val_segmentation_loss: 0.3409 - val_classification_accuracy: 0.5761 - val_segmentation_accuracy: 0.8690\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.8681 - classification_loss: 1.5507 - segmentation_loss: 0.3174 - classification_accuracy: 0.5871 - segmentation_accuracy: 0.8575"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 1.87886 to 1.80469, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 843ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.8681 - classification_loss: 1.5507 - segmentation_loss: 0.3174 - classification_accuracy: 0.5871 - segmentation_accuracy: 0.8575 - val_loss: 1.8047 - val_classification_loss: 1.5061 - val_segmentation_loss: 0.2986 - val_classification_accuracy: 0.6027 - val_segmentation_accuracy: 0.8675\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.7924 - classification_loss: 1.4779 - segmentation_loss: 0.3146 - classification_accuracy: 0.6084 - segmentation_accuracy: 0.8588"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 1.80469 to 1.76247, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 870ms/step\n",
      "11/11 [==============================] - 55s 5s/step - loss: 1.7924 - classification_loss: 1.4779 - segmentation_loss: 0.3146 - classification_accuracy: 0.6084 - segmentation_accuracy: 0.8588 - val_loss: 1.7625 - val_classification_loss: 1.4594 - val_segmentation_loss: 0.3031 - val_classification_accuracy: 0.6115 - val_segmentation_accuracy: 0.8720\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.7250 - classification_loss: 1.4142 - segmentation_loss: 0.3108 - classification_accuracy: 0.6210 - segmentation_accuracy: 0.8606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss improved from 1.76247 to 1.68591, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 830ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.7250 - classification_loss: 1.4142 - segmentation_loss: 0.3108 - classification_accuracy: 0.6210 - segmentation_accuracy: 0.8606 - val_loss: 1.6859 - val_classification_loss: 1.3882 - val_segmentation_loss: 0.2977 - val_classification_accuracy: 0.6145 - val_segmentation_accuracy: 0.8686\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.6547 - classification_loss: 1.3483 - segmentation_loss: 0.3064 - classification_accuracy: 0.6277 - segmentation_accuracy: 0.8618"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 1.68591 to 1.61958, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 839ms/step\n",
      "11/11 [==============================] - 53s 5s/step - loss: 1.6547 - classification_loss: 1.3483 - segmentation_loss: 0.3064 - classification_accuracy: 0.6277 - segmentation_accuracy: 0.8618 - val_loss: 1.6196 - val_classification_loss: 1.3239 - val_segmentation_loss: 0.2957 - val_classification_accuracy: 0.6484 - val_segmentation_accuracy: 0.8707\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.5893 - classification_loss: 1.2848 - segmentation_loss: 0.3045 - classification_accuracy: 0.6577 - segmentation_accuracy: 0.8635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 1.61958 to 1.59204, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 13s 908ms/step\n",
      "11/11 [==============================] - 56s 5s/step - loss: 1.5893 - classification_loss: 1.2848 - segmentation_loss: 0.3045 - classification_accuracy: 0.6577 - segmentation_accuracy: 0.8635 - val_loss: 1.5920 - val_classification_loss: 1.3129 - val_segmentation_loss: 0.2792 - val_classification_accuracy: 0.6366 - val_segmentation_accuracy: 0.8770\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.5905 - classification_loss: 1.2897 - segmentation_loss: 0.3008 - classification_accuracy: 0.6454 - segmentation_accuracy: 0.8646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss improved from 1.59204 to 1.58006, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 836ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.5905 - classification_loss: 1.2897 - segmentation_loss: 0.3008 - classification_accuracy: 0.6454 - segmentation_accuracy: 0.8646 - val_loss: 1.5801 - val_classification_loss: 1.3036 - val_segmentation_loss: 0.2764 - val_classification_accuracy: 0.6189 - val_segmentation_accuracy: 0.8762\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.5159 - classification_loss: 1.2159 - segmentation_loss: 0.3000 - classification_accuracy: 0.6693 - segmentation_accuracy: 0.8652"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss improved from 1.58006 to 1.49083, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 826ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.5159 - classification_loss: 1.2159 - segmentation_loss: 0.3000 - classification_accuracy: 0.6693 - segmentation_accuracy: 0.8652 - val_loss: 1.4908 - val_classification_loss: 1.2096 - val_segmentation_loss: 0.2813 - val_classification_accuracy: 0.6839 - val_segmentation_accuracy: 0.8724\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.5037 - classification_loss: 1.2065 - segmentation_loss: 0.2972 - classification_accuracy: 0.6670 - segmentation_accuracy: 0.8667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss did not improve from 1.49083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 849ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.5037 - classification_loss: 1.2065 - segmentation_loss: 0.2972 - classification_accuracy: 0.6670 - segmentation_accuracy: 0.8667 - val_loss: 1.5495 - val_classification_loss: 1.2630 - val_segmentation_loss: 0.2865 - val_classification_accuracy: 0.6484 - val_segmentation_accuracy: 0.8626\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.4424 - classification_loss: 1.1472 - segmentation_loss: 0.2951 - classification_accuracy: 0.6830 - segmentation_accuracy: 0.8670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss improved from 1.49083 to 1.46871, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 822ms/step\n",
      "11/11 [==============================] - 53s 5s/step - loss: 1.4424 - classification_loss: 1.1472 - segmentation_loss: 0.2951 - classification_accuracy: 0.6830 - segmentation_accuracy: 0.8670 - val_loss: 1.4687 - val_classification_loss: 1.1912 - val_segmentation_loss: 0.2775 - val_classification_accuracy: 0.6839 - val_segmentation_accuracy: 0.8703\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.4259 - classification_loss: 1.1326 - segmentation_loss: 0.2933 - classification_accuracy: 0.6856 - segmentation_accuracy: 0.8677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 1.46871 to 1.43205, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 838ms/step\n",
      "11/11 [==============================] - 53s 5s/step - loss: 1.4259 - classification_loss: 1.1326 - segmentation_loss: 0.2933 - classification_accuracy: 0.6856 - segmentation_accuracy: 0.8677 - val_loss: 1.4320 - val_classification_loss: 1.1564 - val_segmentation_loss: 0.2756 - val_classification_accuracy: 0.6706 - val_segmentation_accuracy: 0.8762\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.3954 - classification_loss: 1.1060 - segmentation_loss: 0.2894 - classification_accuracy: 0.6870 - segmentation_accuracy: 0.8694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 1.43205 to 1.40946, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 834ms/step\n",
      "11/11 [==============================] - 53s 5s/step - loss: 1.3954 - classification_loss: 1.1060 - segmentation_loss: 0.2894 - classification_accuracy: 0.6870 - segmentation_accuracy: 0.8694 - val_loss: 1.4095 - val_classification_loss: 1.1395 - val_segmentation_loss: 0.2700 - val_classification_accuracy: 0.6706 - val_segmentation_accuracy: 0.8770\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.3794 - classification_loss: 1.0899 - segmentation_loss: 0.2895 - classification_accuracy: 0.6973 - segmentation_accuracy: 0.8700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 1.40946 to 1.35741, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 839ms/step\n",
      "11/11 [==============================] - 53s 5s/step - loss: 1.3794 - classification_loss: 1.0899 - segmentation_loss: 0.2895 - classification_accuracy: 0.6973 - segmentation_accuracy: 0.8700 - val_loss: 1.3574 - val_classification_loss: 1.0895 - val_segmentation_loss: 0.2679 - val_classification_accuracy: 0.6913 - val_segmentation_accuracy: 0.8775\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.3469 - classification_loss: 1.0594 - segmentation_loss: 0.2874 - classification_accuracy: 0.7010 - segmentation_accuracy: 0.8706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss improved from 1.35741 to 1.33503, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 831ms/step\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.3469 - classification_loss: 1.0594 - segmentation_loss: 0.2874 - classification_accuracy: 0.7010 - segmentation_accuracy: 0.8706 - val_loss: 1.3350 - val_classification_loss: 1.0701 - val_segmentation_loss: 0.2650 - val_classification_accuracy: 0.7046 - val_segmentation_accuracy: 0.8709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.4163 - classification_loss: 2.0107 - segmentation_loss: 0.4056 - classification_accuracy: 0.4262 - segmentation_accuracy: 0.8196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from 1.33503 to 1.33438, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 13s 844ms/step\n",
      "11/11 [==============================] - 71s 6s/step - loss: 2.4163 - classification_loss: 2.0107 - segmentation_loss: 0.4056 - classification_accuracy: 0.4262 - segmentation_accuracy: 0.8196 - val_loss: 1.3344 - val_classification_loss: 1.0689 - val_segmentation_loss: 0.2655 - val_classification_accuracy: 0.6736 - val_segmentation_accuracy: 0.8748\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.3523 - classification_loss: 1.9547 - segmentation_loss: 0.3976 - classification_accuracy: 0.4339 - segmentation_accuracy: 0.8231"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 1.33438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 834ms/step\n",
      "11/11 [==============================] - 66s 6s/step - loss: 2.3523 - classification_loss: 1.9547 - segmentation_loss: 0.3976 - classification_accuracy: 0.4339 - segmentation_accuracy: 0.8231 - val_loss: 1.3853 - val_classification_loss: 1.1152 - val_segmentation_loss: 0.2702 - val_classification_accuracy: 0.6632 - val_segmentation_accuracy: 0.8740\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.2859 - classification_loss: 1.8929 - segmentation_loss: 0.3931 - classification_accuracy: 0.4599 - segmentation_accuracy: 0.8242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 1.33438 to 1.23131, saving model to best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 828ms/step\n",
      "11/11 [==============================] - 66s 6s/step - loss: 2.2859 - classification_loss: 1.8929 - segmentation_loss: 0.3931 - classification_accuracy: 0.4599 - segmentation_accuracy: 0.8242 - val_loss: 1.2313 - val_classification_loss: 0.9659 - val_segmentation_loss: 0.2654 - val_classification_accuracy: 0.7282 - val_segmentation_accuracy: 0.8784\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.2128 - classification_loss: 1.8313 - segmentation_loss: 0.3816 - classification_accuracy: 0.4719 - segmentation_accuracy: 0.8281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 841ms/step\n",
      "11/11 [==============================] - 66s 6s/step - loss: 2.2128 - classification_loss: 1.8313 - segmentation_loss: 0.3816 - classification_accuracy: 0.4719 - segmentation_accuracy: 0.8281 - val_loss: 1.3058 - val_classification_loss: 1.0328 - val_segmentation_loss: 0.2731 - val_classification_accuracy: 0.7105 - val_segmentation_accuracy: 0.8787\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.1711 - classification_loss: 1.8006 - segmentation_loss: 0.3705 - classification_accuracy: 0.4865 - segmentation_accuracy: 0.8336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 840ms/step\n",
      "11/11 [==============================] - 67s 6s/step - loss: 2.1711 - classification_loss: 1.8006 - segmentation_loss: 0.3705 - classification_accuracy: 0.4865 - segmentation_accuracy: 0.8336 - val_loss: 1.2933 - val_classification_loss: 1.0199 - val_segmentation_loss: 0.2734 - val_classification_accuracy: 0.7105 - val_segmentation_accuracy: 0.8801\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.1360 - classification_loss: 1.7677 - segmentation_loss: 0.3683 - classification_accuracy: 0.4845 - segmentation_accuracy: 0.8331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 850ms/step\n",
      "11/11 [==============================] - 66s 6s/step - loss: 2.1360 - classification_loss: 1.7677 - segmentation_loss: 0.3683 - classification_accuracy: 0.4845 - segmentation_accuracy: 0.8331 - val_loss: 1.2998 - val_classification_loss: 1.0277 - val_segmentation_loss: 0.2721 - val_classification_accuracy: 0.7075 - val_segmentation_accuracy: 0.8824\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.0897 - classification_loss: 1.7300 - segmentation_loss: 0.3597 - classification_accuracy: 0.5012 - segmentation_accuracy: 0.8368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 861ms/step\n",
      "11/11 [==============================] - 66s 6s/step - loss: 2.0897 - classification_loss: 1.7300 - segmentation_loss: 0.3597 - classification_accuracy: 0.5012 - segmentation_accuracy: 0.8368 - val_loss: 1.2806 - val_classification_loss: 1.0017 - val_segmentation_loss: 0.2789 - val_classification_accuracy: 0.7075 - val_segmentation_accuracy: 0.8804\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 2.0211 - classification_loss: 1.6664 - segmentation_loss: 0.3547 - classification_accuracy: 0.5191 - segmentation_accuracy: 0.8384"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 843ms/step\n",
      "11/11 [==============================] - 64s 6s/step - loss: 2.0211 - classification_loss: 1.6664 - segmentation_loss: 0.3547 - classification_accuracy: 0.5191 - segmentation_accuracy: 0.8384 - val_loss: 1.2716 - val_classification_loss: 0.9872 - val_segmentation_loss: 0.2844 - val_classification_accuracy: 0.6898 - val_segmentation_accuracy: 0.8781\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.9729 - classification_loss: 1.6215 - segmentation_loss: 0.3514 - classification_accuracy: 0.5368 - segmentation_accuracy: 0.8411"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 851ms/step\n",
      "11/11 [==============================] - 64s 6s/step - loss: 1.9729 - classification_loss: 1.6215 - segmentation_loss: 0.3514 - classification_accuracy: 0.5368 - segmentation_accuracy: 0.8411 - val_loss: 1.2665 - val_classification_loss: 0.9761 - val_segmentation_loss: 0.2904 - val_classification_accuracy: 0.6957 - val_segmentation_accuracy: 0.8775\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - ETA: 0s - loss: 1.9452 - classification_loss: 1.5956 - segmentation_loss: 0.3496 - classification_accuracy: 0.5365 - segmentation_accuracy: 0.8419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss did not improve from 1.23131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivervu25/tf-env/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 12s 816ms/step\n",
      "11/11 [==============================] - 63s 6s/step - loss: 1.9452 - classification_loss: 1.5956 - segmentation_loss: 0.3496 - classification_accuracy: 0.5365 - segmentation_accuracy: 0.8419 - val_loss: 1.3437 - val_classification_loss: 1.0494 - val_segmentation_loss: 0.2942 - val_classification_accuracy: 0.6942 - val_segmentation_accuracy: 0.8769\n"
     ]
    }
   ],
   "source": [
    "save_callback = CustomSaveCallback(\"tf-env/mxb362_models\", test_class_seg)\n",
    "\n",
    "# Initialize ModelCheckpoint\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# Initially freeze the base model\n",
    "mobile_base.trainable = False\n",
    "\n",
    "# Compile the model with appropriate loss functions\n",
    "model5.compile(optimizer='adam',\n",
    "               loss={'classification': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                     'segmentation': tf.keras.losses.BinaryCrossentropy(from_logits=True)},\n",
    "               metrics={'classification': 'accuracy',\n",
    "                        'segmentation': 'accuracy'})\n",
    "\n",
    "\n",
    "history5_1 = model5.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, save_callback]\n",
    ")\n",
    "\n",
    "save_callback = CustomSaveCallback(\"tf-env/mxb362_models\", test_class_seg, epoch_offset=20)\n",
    "\n",
    "# Unfreeze the base model\n",
    "mobile_base.trainable = True\n",
    "\n",
    "# Re-compile the model\n",
    "model5.compile(optimizer=tf.keras.optimizers.Adam(0.00001),  # Lower learning rate\n",
    "               loss={'classification': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                     'segmentation': tf.keras.losses.BinaryCrossentropy(from_logits=True)},\n",
    "               metrics={'classification': 'accuracy',\n",
    "                        'segmentation': 'accuracy'})\n",
    "\n",
    "\n",
    "history5_2 = model5.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, save_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b85566-e8df-4b51-aa21-5b307d26fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 15s 984ms/step - loss: 1.6749 - classification_loss: 1.3857 - segmentation_loss: 0.2892 - classification_accuracy: 0.5824 - segmentation_accuracy: 0.8739\n",
      "Test accuracy: 58.24475288391113%\n"
     ]
    }
   ],
   "source": [
    "test_class_seg = load_oxford_pets('test', shuffle=False, batch_size=batch_size, image_size=128)\n",
    "\n",
    "checkpoint_path = \"best_model_2.h5\" \n",
    "\n",
    "best_model_2 = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "loss, class_loss, seg_loss, class_accuracy, seg_accuracy = best_model_2.evaluate(test_class_seg)\n",
    "\n",
    "print(f\"Test accuracy: {class_accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ace9451-778c-462f-a2e6-ec54f7c755e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 128, 128, 3)\n",
      "(273, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for img, label in test_class_seg.take(1):\n",
    "    print(img.shape)  # Should ideally match the model's input shape\n",
    "    print(label.shape)  # Check if this matches the [4472832] from the error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b27a9cda-1171-4679-8700-f20a41e5c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 14s 942ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predictions from the model\n",
    "class_preds, seg_preds = best_model_2.predict(test_class_seg)\n",
    "true_masks = np.concatenate([y[1].numpy() for x, y in test_class_seg], axis=0)\n",
    "\n",
    "# Flatten the prediction and ground truth arrays and binarize them\n",
    "seg_preds_flat = seg_preds.reshape(-1) > 0.5\n",
    "true_masks_flat = true_masks.reshape(-1) > 0.5\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_masks_flat, seg_preds_flat)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='viridis')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edd809-d49d-4380-8529-74d13e2a9ff1",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff64ab4-bbbb-4e8e-9f22-2d733687169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Segmentation Accuracy</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>True Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.721536</td>\n",
       "      <td>19092375</td>\n",
       "      <td>15556271</td>\n",
       "      <td>2252539</td>\n",
       "      <td>23211711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.814182</td>\n",
       "      <td>24132584</td>\n",
       "      <td>10559295</td>\n",
       "      <td>3459956</td>\n",
       "      <td>21961061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>27851948</td>\n",
       "      <td>6792096</td>\n",
       "      <td>5420155</td>\n",
       "      <td>20048697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.844253</td>\n",
       "      <td>28905045</td>\n",
       "      <td>5764261</td>\n",
       "      <td>5938947</td>\n",
       "      <td>19504643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>29261196</td>\n",
       "      <td>5356578</td>\n",
       "      <td>6041328</td>\n",
       "      <td>19453794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.850901</td>\n",
       "      <td>30246720</td>\n",
       "      <td>4414126</td>\n",
       "      <td>6948109</td>\n",
       "      <td>18503941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.854176</td>\n",
       "      <td>29325699</td>\n",
       "      <td>5311861</td>\n",
       "      <td>5896899</td>\n",
       "      <td>19578437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.857476</td>\n",
       "      <td>30756789</td>\n",
       "      <td>3898383</td>\n",
       "      <td>7362063</td>\n",
       "      <td>18095661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.858764</td>\n",
       "      <td>30127353</td>\n",
       "      <td>4524237</td>\n",
       "      <td>6673683</td>\n",
       "      <td>18787623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.860590</td>\n",
       "      <td>30615035</td>\n",
       "      <td>4077306</td>\n",
       "      <td>7076956</td>\n",
       "      <td>18343599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.861766</td>\n",
       "      <td>30156810</td>\n",
       "      <td>4499469</td>\n",
       "      <td>6792451</td>\n",
       "      <td>18664166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.863496</td>\n",
       "      <td>30456889</td>\n",
       "      <td>4189895</td>\n",
       "      <td>7010662</td>\n",
       "      <td>18455450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.864644</td>\n",
       "      <td>30466985</td>\n",
       "      <td>4192915</td>\n",
       "      <td>7016161</td>\n",
       "      <td>18436835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.865246</td>\n",
       "      <td>30883904</td>\n",
       "      <td>3756681</td>\n",
       "      <td>7357968</td>\n",
       "      <td>18114343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.866661</td>\n",
       "      <td>31556297</td>\n",
       "      <td>3087213</td>\n",
       "      <td>8361316</td>\n",
       "      <td>17108070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.867005</td>\n",
       "      <td>31136032</td>\n",
       "      <td>3526090</td>\n",
       "      <td>7827932</td>\n",
       "      <td>17622842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.867679</td>\n",
       "      <td>30391210</td>\n",
       "      <td>4271300</td>\n",
       "      <td>6861519</td>\n",
       "      <td>18588867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.869352</td>\n",
       "      <td>30912294</td>\n",
       "      <td>3738007</td>\n",
       "      <td>7453166</td>\n",
       "      <td>18009429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.870034</td>\n",
       "      <td>30882731</td>\n",
       "      <td>3788121</td>\n",
       "      <td>7464914</td>\n",
       "      <td>17977130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.870623</td>\n",
       "      <td>31401782</td>\n",
       "      <td>3264469</td>\n",
       "      <td>8081167</td>\n",
       "      <td>17365478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.819565</td>\n",
       "      <td>31094607</td>\n",
       "      <td>3562892</td>\n",
       "      <td>7789551</td>\n",
       "      <td>17665846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.823137</td>\n",
       "      <td>30931904</td>\n",
       "      <td>3745078</td>\n",
       "      <td>7363205</td>\n",
       "      <td>18072709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.824190</td>\n",
       "      <td>30718688</td>\n",
       "      <td>3916339</td>\n",
       "      <td>7196423</td>\n",
       "      <td>18281446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.828083</td>\n",
       "      <td>30485019</td>\n",
       "      <td>4166477</td>\n",
       "      <td>6892126</td>\n",
       "      <td>18569274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.833593</td>\n",
       "      <td>30280497</td>\n",
       "      <td>4360003</td>\n",
       "      <td>6748923</td>\n",
       "      <td>18723473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>30133072</td>\n",
       "      <td>4529145</td>\n",
       "      <td>6472441</td>\n",
       "      <td>18978238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.836811</td>\n",
       "      <td>29979889</td>\n",
       "      <td>4679744</td>\n",
       "      <td>6421655</td>\n",
       "      <td>19031608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.838359</td>\n",
       "      <td>29888535</td>\n",
       "      <td>4767505</td>\n",
       "      <td>6338035</td>\n",
       "      <td>19118821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.841053</td>\n",
       "      <td>29786295</td>\n",
       "      <td>4869634</td>\n",
       "      <td>6230655</td>\n",
       "      <td>19226312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.841855</td>\n",
       "      <td>29640995</td>\n",
       "      <td>5004283</td>\n",
       "      <td>6188514</td>\n",
       "      <td>19279104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Segmentation Accuracy  True Positives  False Positives  \\\n",
       "0       1               0.721536        19092375         15556271   \n",
       "1       2               0.814182        24132584         10559295   \n",
       "2       3               0.837079        27851948          6792096   \n",
       "3       4               0.844253        28905045          5764261   \n",
       "4       5               0.847153        29261196          5356578   \n",
       "5       6               0.850901        30246720          4414126   \n",
       "6       7               0.854176        29325699          5311861   \n",
       "7       8               0.857476        30756789          3898383   \n",
       "8       9               0.858764        30127353          4524237   \n",
       "9      10               0.860590        30615035          4077306   \n",
       "10     11               0.861766        30156810          4499469   \n",
       "11     12               0.863496        30456889          4189895   \n",
       "12     13               0.864644        30466985          4192915   \n",
       "13     14               0.865246        30883904          3756681   \n",
       "14     15               0.866661        31556297          3087213   \n",
       "15     16               0.867005        31136032          3526090   \n",
       "16     17               0.867679        30391210          4271300   \n",
       "17     18               0.869352        30912294          3738007   \n",
       "18     19               0.870034        30882731          3788121   \n",
       "19     20               0.870623        31401782          3264469   \n",
       "20     21               0.819565        31094607          3562892   \n",
       "21     22               0.823137        30931904          3745078   \n",
       "22     23               0.824190        30718688          3916339   \n",
       "23     24               0.828083        30485019          4166477   \n",
       "24     25               0.833593        30280497          4360003   \n",
       "25     26               0.833060        30133072          4529145   \n",
       "26     27               0.836811        29979889          4679744   \n",
       "27     28               0.838359        29888535          4767505   \n",
       "28     29               0.841053        29786295          4869634   \n",
       "29     30               0.841855        29640995          5004283   \n",
       "\n",
       "    False Negatives  True Negatives  \n",
       "0           2252539        23211711  \n",
       "1           3459956        21961061  \n",
       "2           5420155        20048697  \n",
       "3           5938947        19504643  \n",
       "4           6041328        19453794  \n",
       "5           6948109        18503941  \n",
       "6           5896899        19578437  \n",
       "7           7362063        18095661  \n",
       "8           6673683        18787623  \n",
       "9           7076956        18343599  \n",
       "10          6792451        18664166  \n",
       "11          7010662        18455450  \n",
       "12          7016161        18436835  \n",
       "13          7357968        18114343  \n",
       "14          8361316        17108070  \n",
       "15          7827932        17622842  \n",
       "16          6861519        18588867  \n",
       "17          7453166        18009429  \n",
       "18          7464914        17977130  \n",
       "19          8081167        17365478  \n",
       "20          7789551        17665846  \n",
       "21          7363205        18072709  \n",
       "22          7196423        18281446  \n",
       "23          6892126        18569274  \n",
       "24          6748923        18723473  \n",
       "25          6472441        18978238  \n",
       "26          6421655        19031608  \n",
       "27          6338035        19118821  \n",
       "28          6230655        19226312  \n",
       "29          6188514        19279104  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to hold individual dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each epoch\n",
    "for epoch in range(1, 31):\n",
    "    # Define the filepath based on the epoch\n",
    "    filepath = f'tf-env/mxb362_models/segmentation_metrics_epoch_{epoch}.csv'\n",
    "    \n",
    "    # Read the current file\n",
    "    current_data = pd.read_csv(filepath)\n",
    "    \n",
    "    # Add the current data to the list of dataframes\n",
    "    dataframes.append(current_data)\n",
    "\n",
    "# Concatenate all the dataframes in the list\n",
    "all_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "\n",
    "all_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
